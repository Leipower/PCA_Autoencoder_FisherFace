# -*- coding: utf-8 -*-
"""Autoencoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vYyWiR6FsQFVmzTcEbTiTaInyjBbChsG
"""

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

!rm mywarper.py
!rm ae.py

from google.colab import files
src = list(files.upload().values())[0]

!pip install torch
!pip install torchvision
!pip install --no-cache-dir -I pillow
!pip install imageio

import os
import numpy as np
import argparse
import matplotlib.pyplot as plt
import skimage
from skimage import io, transform
import scipy.io as sio
from mywarper import warp 

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils

class args(object):
    def __init__(self):
        self.epochs = 5
        self.batch_size = 100
        self.seed = 12345
        self.device = 0
        self.image_dir = '/content/drive/My Drive/Colab Notebooks/data/images/'
        self.landmark_dir = '/content/drive/My Drive/Colab Notebooks/data/landmarks/'
        self.male_img_dir = '/content/drive/My Drive/Colab Notebooks/data/male_images/'
        self.female_img_dir = '/content/drive/My Drive/Colab Notebooks/data/female_images/'
        self.male_landmark = '/content/drive/My Drive/Colab Notebooks/data/male_landmarks/'
        self.female_landmark = '/content/drive/My Drive/Colab Notebooks/data/female_landmarks/'
        self.path = '/content/drive/My Drive/Colab Notebooks/data/model/'
        self.log = '/content/drive/My Drive/Colab Notebooks/data/log/'
        self.appear_lr = 7e-4
        self.landmark_lr = 1e-4
        self.cuda = True
        
args = args()

# Read Dataset
class data_reader(object):
    def __init__(self, root_dir, file_str_len, origin_name, file_format):
        self.root_dir = root_dir
        self.file_str_len = file_str_len
        self.origin_name = origin_name
        self.file_format = file_format

    def read(self, split, read_type):
        files_len = len([name for name in os.listdir(self.root_dir) 
                        if os.path.isfile(os.path.join(self.root_dir, name))])
        counter = 0
        idx = counter
        dataset = []
        train_dataset = []
        test_dataset = []
        while counter < files_len:
            name = self.origin_name + str(idx)
            if len(name) > self.file_str_len:
                name = name[len(name)-self.file_str_len:]
            try:
                if read_type == 'image':
                    data = io.imread(self.root_dir + name + self.file_format)
                elif read_type == 'landmark':
                    mat_data = sio.loadmat(self.root_dir + name + self.file_format)

                    data = mat_data['lms']
                dataset.append(data)
                counter += 1
            except FileNotFoundError:
                pass
            idx += 1
        train_dataset = dataset[:split]
        test_dataset = dataset[split:]
        return train_dataset, test_dataset

# Construct Dataset
class ImgToTensor(object):
    def __call__(self, sample):
        sample = sample.transpose((2, 0, 1))
        return torch.tensor(sample, dtype=torch.float32)/255

class LandmarkToTensor(object):
    def __call__(self, sample):
        return torch.tensor(sample, dtype=torch.float32)/128

class dataset_constructor(Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        sample_data = self.dataset[idx]
        if self.transform:
            sample_data = self.transform(sample_data)
        return sample_data

args.cuda = torch.cuda.is_available()
torch.cuda.set_device(args.device)

if not os.path.exists(args.path):
    os.makedirs(args.path)
if not os.path.exists(args.log):
    os.makedirs(args.log)
if args.cuda:
    torch.cuda.manual_seed(args.seed)

class appearance_autoencoder(nn.Module):
    def __init__(self):
        super(appearance_autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            # TODO: Fill in the encoder structure
            nn.Conv2d(3, 16, kernel_size = 3,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.Conv2d(16, 32, kernel_size = 3,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.Conv2d(32, 64, kernel_size = 3,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.Conv2d(64, 128, kernel_size = 3,stride = 2,padding = 1),
            nn.LeakyReLU(),
        )

        self.fc1 = nn.Sequential(                    
            # TODO: Fill in the FC layer structure
            nn.Linear(128*8*8,50),
            nn.LeakyReLU(),
        )

        self.decoder = nn.Sequential(
            # TODO: Fill in the decoder structure
            # Hint: De-Conv in PyTorch: ConvTranspose2d
            nn.ConvTranspose2d(50, 128, kernel_size = 8,stride = 1),
            nn.LeakyReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size = 4,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.ConvTranspose2d(64, 32, kernel_size = 4,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.ConvTranspose2d(32, 16, kernel_size = 4,stride = 2,padding = 1),
            nn.LeakyReLU(),
            nn.ConvTranspose2d(16, 3, kernel_size = 4,stride = 2,padding = 1),
            nn.Sigmoid(),
        )

    def forward(self, x):
            # TODO: Fill in forward pass
            x = self.encoder(x)
            z = self.fc1(x.view(-1,128*8*8))
            x_recon = self.decoder(z.view(-1, 50, 1, 1))
            return x_recon,z
          
    def given_latent(self, x):
        x = x.view(-1,50,1,1)
        x_recon = self.decoder(x)
        return x_recon

class landmark_autoencoder(nn.Module):
    def __init__(self):
        super(landmark_autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            # TODO: Fill in the encoder structure
            nn.Linear(68*2, 100),
            nn.LeakyReLU(),
            nn.Linear(100, 10),
            nn.LeakyReLU(),
        )
        self.decoder = nn.Sequential(
            # TODO: Fill in the decoder structure
            nn.Linear(10, 100),
            nn.LeakyReLU(),
            nn.Linear(100, 68*2),
            nn.Sigmoid(),
        )

    def forward(self, x):
            # TODO: Fill in forward pass
            x = x.view(-1,68*2)
            z = self.encoder(x)
            x_recon = self.decoder(z)
            x_recon = x_recon.view(-1,68,2)
            return x_recon,z
    
    def given_latent(self, x):
        x_recon = self.decoder(x)
        x_recon = x_recon.view(-1,68,2)
        return x_recon

class autoencoder(object):
    def __init__(self, appear_lr, landmark_lr, use_cuda):
        self.appear_model = appearance_autoencoder()
        self.landmark_model = landmark_autoencoder()
        self.use_cuda = use_cuda
        if use_cuda:
            self.appear_model.cuda()
            self.landmark_model.cuda()
        self.criterion = nn.MSELoss()
        self.appear_optim = optim.Adam(self.appear_model.parameters(), lr=appear_lr)
        self.landmark_optim = optim.Adam(self.landmark_model.parameters(), lr=landmark_lr)
        
    def train_appear_model(self, epochs, trainloader):
        self.appear_model.train()
        epoch = 0
        # TODO: Train appearance autoencoder
        for epoch in range(0, epochs):
            training_loss = 0
            for x in trainloader:
                if self.use_cuda:
                    x = x.cuda()

                self.appear_optim.zero_grad()
                x_recon,z = self.appear_model(x)
                loss = self.criterion(x_recon, x)
                loss.backward()
                self.appear_optim.step()

                training_loss += loss.item()
            print('Image Training Epoch:{}, Loss:{:.6f}'.format(epoch, training_loss/len(trainloader)))

    def train_landmark_model(self, epochs, trainloader):
        self.landmark_model.train()
        epoch = 0
        # TODO: Train landmark autoencoder
        training_loss = 0
        for epoch in range(0, epochs):
            training_loss = 0
            for x in trainloader:
                if self.use_cuda:
                    x = x.cuda()
            
                self.landmark_optim.zero_grad()
                x_recon,z = self.landmark_model(x)
                loss = self.criterion(x_recon, x)
                loss.backward()
                self.landmark_optim.step()

                training_loss += loss.item()
            print('Landmark Training Epoch:{}, Loss:{:.6f}'.format(epoch, training_loss/len(trainloader)))

    def test_appear_model(self, testloader):
        self.appear_model.eval()
        # TODO: Test appearance autoencoder
        testing_loss = 0
        appear_recon = []
        appear_latent = []
        for x in testloader:
          if self.use_cuda:
            x = x.cuda()
            
          x_recon,z = self.appear_model(x)
          loss = self.criterion(x_recon,x)
          testing_loss += loss.item()
          appear_recon.append(x_recon.data.cpu().numpy())
          appear_latent.append(z.data.cpu().numpy())
        #print('Landmark Training Epoch:{}, Loss:{:.6f}'.format(testing_loss/len(testloader)))
        return(appear_recon,appear_latent,testing_loss)
    
    def test_landmark_model(self, testloader):
        self.appear_model.eval()
        # TODO: Test appearance autoencoder
        testing_loss = 0
        landmark_recon = []
        landmark_latent = []
        for x in testloader:
          if self.use_cuda:
            x = x.cuda()
            
          x_recon,z = self.landmark_model(x)
          loss = self.criterion(x_recon,x)
          testing_loss += loss.item()
          landmark_recon.append(x_recon.data.cpu().numpy())
          landmark_latent.append(z.data.cpu().numpy())
        #print('Landmark Training Epoch:{}, Loss:{:.6f}'.format(testing_loss/len(testloader)))
        return(landmark_recon,landmark_latent,testing_loss)
        
    def appear_given_fc(self, fcloader):
      self.appear_fc_recon = []
      for fc in fcloader:
        if self.use_cuda:
          fc = fc.cuda()
        recon = self.appear_model.given_latent(fc)
        self.appear_fc_recon.append(recon.data.cpu().numpy())
      
    def landmark_given_fc(self, fcloader):
      self.landmark_fc_recon = []
      for fc in fcloader:
        if self.use_cuda:
          fc = fc.cuda()
        recon = self.landmark_model.given_latent(fc)
        self.landmark_fc_recon.append(recon.data.cpu().numpy())

face_images_reader = data_reader(args.image_dir, 6, '000000', '.jpg')
face_landmark_reader = data_reader(args.landmark_dir, 6, '000000', '.mat')
face_images_train, face_images_test = face_images_reader.read(split=800,read_type='image')
face_landmark_train, face_landmark_test = face_landmark_reader.read(split=800,read_type='landmark')

face_landmark_train_copy = face_landmark_train
mean_lm_train=np.asarray(face_landmark_train_copy).reshape((800,-1)).mean(axis=0)

warp_face_images_train = []
warp_face_images_test = []

for i in range(800):
  warp_face_images_train.append(warp(face_images_train[i],face_landmark_train[i],mean_lm_train.reshape(68,2)))

for i in range(200):
  warp_face_images_test.append(warp(face_images_test[i],face_landmark_test[i],mean_lm_train.reshape(68,2)))

face_trainset = dataset_constructor(warp_face_images_train, transform=transforms.Compose([ImgToTensor()]))
face_testset = dataset_constructor(warp_face_images_test, transform=transforms.Compose([ImgToTensor()]))
face_trainloader = torch.utils.data.DataLoader(face_trainset,batch_size=args.batch_size,shuffle=True,num_workers=0)
face_testloader = torch.utils.data.DataLoader(face_testset,batch_size=args.batch_size,shuffle=False,num_workers=0)

landmark_trainset = dataset_constructor(face_landmark_train, transform=transforms.Compose([LandmarkToTensor()]))
landmark_testset = dataset_constructor(face_landmark_test, transform=transforms.Compose([LandmarkToTensor()]))
landmark_trainloader = torch.utils.data.DataLoader(landmark_trainset,batch_size=args.batch_size,shuffle=True,num_workers=0)
landmark_testloader = torch.utils.data.DataLoader(landmark_testset,batch_size=args.batch_size,shuffle=False,num_workers=0)
a = autoencoder(args.appear_lr,args.landmark_lr,args.cuda)

a.train_appear_model(epochs=300,trainloader=face_trainloader)

a.train_landmark_model(epochs=300,trainloader=landmark_trainloader)

recon_img,recon_latent,img_loss = a.test_appear_model(face_testloader)
recon_lm,lm_latent,lm_loss = a.test_landmark_model(landmark_testloader)

np.swapaxes(recon_img[0][1,:,:,:],0,2).shape

recon_img_plot = []
for i in range(100):
  im = np.swapaxes(recon_img[0][i,:,:,:],0,2)
  recon_img_plot.append(warp(im,mean_lm_train.reshape(68,2),recon_lm[0][i,:,:]))
for i in range(100):
  im = np.swapaxes(recon_img[1][i,:,:,:],0,2)
  recon_img_plot.append(warp(im,mean_lm_train.reshape(68,2),recon_lm[1][i,:,:]))
  
recon_img_plot = np.asarray(recon_img_plot)
recon_img_plot.shape

from mywarper import plot

plot(recon_img_plot,20,10,3,128,128)

img_loss, lm_loss

recon_latent[0].shape
np.asarray(recon_latent[0]).dtype
lm_fc.shape

recon_fc = np.vstack(np.concatenate((np.asarray(recon_latent[0]),np.asarray(recon_latent[1]))))
lm_fc = np.vstack(np.concatenate((np.asarray(lm_latent[0]),np.asarray(lm_latent[1]))))

# The mean and variance of the FCs
appear_fc_mean = np.zeros(50)
appear_fc_variance = np.zeros(50)
for i in range(50):
  appear_fc_mean[i] = np.mean(recon_fc[:,i])
  appear_fc_variance[i] = np.var(recon_fc[:,i])
fc_index = np.arange(50)
appear_order = appear_fc_variance.argsort()

lm_fc_mean = np.zeros(10)
lm_fc_variance = np.zeros(10)
for i in range(10):
  lm_fc_mean[i] = np.mean(lm_fc[:,i])
  lm_fc_variance[i] = np.var(lm_fc[:,i])
lm_fc_index = np.arange(10)
lm_order = lm_fc_variance.argsort()
# Select the image for interpolation
select_face = warp_face_images_test[1]
select_lm = face_landmark_test[1]
select_appear_fc = recon_fc[1,:] 
select_lm_fc = lm_fc[1,:]

plt.rcParams["axes.grid"] = False
plt.imshow(select_face)
plt.show()

for i in range(4):
  index = fc_index[appear_order==i]
  appear_fc_mat = np.repeat([select_appear_fc], 10, axis = 0)
  for j in range(10):
    appear_fc_mat[j][index] = np.random.normal(appear_fc_mean[index], np.sqrt(appear_fc_variance[index])*4)
  appear_fc_mat = torch.utils.data.DataLoader(torch.tensor(appear_fc_mat,dtype=torch.float32),batch_size=args.batch_size,shuffle=True,num_workers=0)
  a.appear_given_fc(appear_fc_mat)
  appear_list = np.array(a.appear_fc_recon[0]).transpose(0,2,3,1)
  plot(appear_list,1,10,3,128,128)

meanlm = mean_lm_train.reshape(68,2)

lm_list = []
for i in range(2):
  index = lm_fc_index[lm_order==i]
  lm_fc_mat = np.repeat([select_lm_fc], 10, axis = 0)
  for j in range(10):
    lm_fc_mat[j][index] = np.random.normal(lm_fc_mean[index], np.sqrt(lm_fc_variance[index])*300)
  lm_fc_mat = torch.utils.data.DataLoader(torch.tensor(lm_fc_mat,dtype=torch.float32),batch_size=args.batch_size,shuffle=True,num_workers=0)
  a.landmark_given_fc(lm_fc_mat)
  lm_list.append(np.array(a.landmark_fc_recon[0]))
  
fig1=plt.figure(figsize=(20, 2))
columns = 10
rows = 1
for i in range(1, columns*rows+1):
    fig1.add_subplot(rows, columns, i)
    plt.scatter(np.array(lm_list[0])[i-1,:,0]*128,np.array(lm_list[0])[i-1,:,1]*128)
    plt.gca().invert_yaxis()
plt.show()

fig2=plt.figure(figsize=(20, 2))
columns = 10
rows = 1
for i in range(1, columns*rows+1):
    fig2.add_subplot(rows, columns, i)
    plt.scatter(np.array(lm_list[1])[i-1,:,0]*128,np.array(lm_list[1])[i-1,:,1]*128)
    plt.gca().invert_yaxis()
plt.show()

face_landmark_test[1]
lm_fc_mean

fig1=plt.figure(figsize=(20, 2))
columns = 10
rows = 1
for i in range(1, columns*rows+1):
    lm = np.array(lm_list[0][i-1,:,:])*128
    testplot = warp(select_face,meanlm,lm)
    fig1.add_subplot(rows, columns, i)
    plt.rcParams["axes.grid"] = False
    plt.imshow(testplot)
plt.show()

fig2=plt.figure(figsize=(20, 2))
columns = 10
rows = 1
for i in range(1, columns*rows+1):
    lm = np.array(lm_list[1][i-1,:,:])*128
    testplot = warp(select_face,meanlm,lm)
    fig2.add_subplot(rows, columns, i)
    plt.rcParams["axes.grid"] = False
    plt.imshow(testplot)
plt.show()